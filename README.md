# Grid Search Hyper-Parameter Tuning and K-Means Clustering toImprove the Decision Tree Accuracy
Machine learning(Research-based Project)

Predicting a correct decision on data set is very much important, it can be in any field either in business, engineering, civil planning, law or in other real-life areas. 
The base of decision making ishaving the correct data, a dataset is the collection of related discrete items of related data that may be accessed individually or in combination or managed as whole entity. The dataset which we are dealing with our research experiment is customer churn that is a binary ('Yes'/'No') classification problem, in this we are predicting the customer churn for the business. Here the churn prediction is about detecting which customers are likely to 
leave the business. The algorithm that works best on such binary classification problems is logistic regression, which  gives the decent accuracy around 85%, but this accuracy is not enough to make correct decisions in a business. 
Therefore, we are introducing a hybrid algorithm that uses Decision Tree as the classifier, in which clustering and hyper-parameter tuning are used as the supportive elements.
Although the accuracy of decisiontree on this customer churn dataset is very less, near to 75%. It is because the Decision Tree algorithm have many limitations like the independency between samples which has drastic effects on its accuracy, along with this it is not suitable for big size datasets. Decision Tree causes overfitting problem for the model in case data does not fit well. For the setup of our hybrid algorithm the data filtration is used in the initial phase, it is the refining of data which are either null values or values that are not supported by Decision Tree algorithm. After the data filtration, clustering is used to group the data elements 
on the basis of similarity and dissimilarity and makes the data more structured which makes Decision Tree to perform better. After using clustering outcome with the Decision tree, it shows a little hike and the accuracy reached near to that oflogistic regression, but there is still a large possibility for the improvement. The next step of our hybrid algorithm is hyperparameter tuning which pull out the bad and irrelevantparameters forthe churn prediction. After using this on the outcome of clustering, the accuracy of Decision Tree gets improved and touches the 90% banner. As the customer churn dataset has only two possible outcomes, i.e. 'Yes'/'No'. So due to the nature of our dataset we didn't take much benefit ofclustering but the results become more better.This hybrid algorithm is applied on 20 datasets and 
there is a large benefit of clustering for the datasets which has several class values that are to be predicted. Our customer churn dataset has only two class values so we used hyper-parameter tuning on this dataset without other customizationand the Decision Tree accuracy touched the 95% banner.
The hybrid algorithm has improved the accuracy of Decision Tree by more than 25%. So, this can be concluded that the achieved results of experiments are far greater than the best suited machine learning model (logistic regression) on this binary classification dataset.
